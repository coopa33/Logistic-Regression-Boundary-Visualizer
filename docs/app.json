[{"name": "app.py", "content": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nfrom shiny import render, reactive\nfrom shiny.express import ui, input\nfrom shared import extract_feature_names, drop_features, read_structure_data, transform_array, product_remainder\nfrom gradient_descent import gradient_descent\n\n### Render structured Dataframe ###\nui.input_file(\"csv_data\", \"Choose CSV file\", accept=[\".csv\"], multiple=False)\n\n@reactive.calc\ndef parsed_file():\n    file: list[FileInfo] | None = input.csv_data()\n    if file is None:\n        return pd.DataFrame()\n    return pd.read_csv(file[0][\"datapath\"])\n\n\nui.input_selectize(\n        \"features_exclude\", \n        \"Select features to be excluded from subsequent transformation\",\n        choices = [],\n        multiple = True)\n\nui.input_selectize(\n    \"y_select\",\n    \"Select dependent variable\",\n    choices = [],\n    multiple = True)\n\nui.input_selectize(\n        \"one_hot_select\",\n        \"Select categorical variables to represent as one-hot\",\n        choices = [],\n        multiple = True)\n\nui.input_selectize(\n        \"feature_plot\",\n        \"Select features to plot\",\n        choices = [],\n        multiple = True)\n\n\n\"\"\"\nSelection of features is paramount to the data cleaning functioning properly. Currently the user must chose the features to exclude, feature to use as y, and features to one-hot. If by the user choice, there remains features that need but are not selected (f.e. a categorical feature with strings, which needs to be one-hot encoded) then the resulting cleaned and normalized data will not render. Therefore choosing features correctly must be done before rendering the data and decision boundary plots. \n\"\"\"\n@reactive.effect\ndef update_exclude_features():\n    df = parsed_file()\n    cols = list(df.columns)\n    ui.update_selectize(\"features_exclude\", choices=cols)\n\n@reactive.effect\ndef update_y_select_choices():\n    first_val = input.features_exclude()\n    if first_val is None:\n        first_val = []\n    y_choice = {feature: feature for feature in list(parsed_file().columns) if feature not in first_val}\n    ui.update_selectize(\"y_select\", choices = y_choice, server=True)\n\n\n@reactive.effect\ndef update_one_hot_plot():\n    first_val = input.features_exclude()\n    second_val = input.y_select()\n    if first_val is None:\n        first_val = []\n    if second_val is None:\n        second_val = []\n    onehot_choice = {feature: feature for feature in list(parsed_file().columns) if feature not in first_val and feature not in second_val}\n    ui.update_selectize(\"one_hot_select\", choices = onehot_choice, server=True)\n\n@reactive.effect\ndef update_feature_selection():\n    df = parsed_file()\n    features_to_drop = list(input.features_exclude())\n    y = list(input.y_select())\n    one_hot = list(input.one_hot_select())\n    if not (features_to_drop == [] or y == [] or one_hot == []):\n        try:\n            df_cleaned = read_structure_data(df, y[0], features_to_drop, one_hot)\n            feature_choice = {i: list(df_cleaned.columns)[i] for i in range(len(list(df_cleaned.columns)))}\n            ui.update_selectize(\"feature_plot\", choices = feature_choice)\n        except:\n            pass \n    \n\n@render.data_frame\ndef exclude_features_df():\n    df = parsed_file()\n    features_to_drop = list(input.features_exclude())\n    if len(features_to_drop)<1:\n        return render.DataGrid(df)\n    else:\n        df = drop_features(df, features_to_drop)\n        return render.DataGrid(df)\n    \n@render.data_frame\ndef cleaned_df():\n    df = parsed_file()\n    features_to_drop = list(input.features_exclude())\n    y = list(input.y_select())\n    one_hot = list(input.one_hot_select())\n    if not (features_to_drop == [] or y == [] or one_hot == []):\n        try:\n            df_cleaned = read_structure_data(df, y[0], features_to_drop, one_hot)\n            return render.DataGrid(df_cleaned)\n        except:\n            # If features remain that need to be one-hot transformed, only the original dataframe will be rendered\n            return render.DataGrid(parsed_file())\n\n@render.plot\ndef plot_data():\n    df = parsed_file()\n    features_to_drop = list(input.features_exclude())\n    y = list(input.y_select())\n    one_hot = list(input.one_hot_select())\n    features_select = list(input.feature_plot())\n\n    if not (features_to_drop == [] or y == [] or one_hot == [] or len(features_select) != 2):\n\n        # Clean data and extract new feature names\n        df_cleaned = read_structure_data(df, y[0], features_to_drop, one_hot)\n        feature_names = list(df_cleaned.columns)\n\n        # Gradient descent\n        X_array, y_array = transform_array(df_cleaned, df_cleaned.columns.get_loc(y[0]))\n        w_init = np.random.rand(X_array.shape[1])\n        b_init = np.random.rand()\n        w_hat, b_hat = gradient_descent(X_array, y_array, w_init, b_init)\n\n        # Average remaining features, factor coefficients and sum, and add to intercept\n        b_hat += product_remainder(X_array, w_hat, [int(features_select[0]), int(features_select[1])])\n        \n        # Plot\n        fig, ax = plt.subplots()\n        x_ax = df_cleaned[feature_names[int(features_select[0])]]\n        y_ax = df_cleaned[feature_names[int(features_select[1])]]\n        x_boundary = np.linspace(min(x_ax), max(x_ax), 100)\n        y_boundary = (-x_boundary * w_hat[int(features_select[0])] - b_hat) / w_hat[int(features_select[1])]\n        ax.scatter(x_ax, y_ax, c = df_cleaned[y[0]])\n        ax.plot(x_boundary, y_boundary, color = \"red\")\n        ax.set_title(\"Distribution of features\")\n        ax.set_xlabel(feature_names[int(features_select[0])])\n        ax.set_ylabel(feature_names[int(features_select[1])])\n        return fig\n\n\n    \n    \n    \n", "type": "text"}, {"name": "gradient_descent.py", "content": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef z_value(X, w, b):\n    \"\"\"\n    Calculate z-value for linear term\n\n    Parameters\n    ----------\n    X: ndarray\n        2-D ndarray of datapoints (observations, features)\n    w: ndarray\n        1-D ndarray of feature coefficients\n    b: float\n        Intercept \n\n    Returns\n    -------\n    z: ndarray\n        2-D column vector of z-values for each observation (observations, 1)\n    \"\"\"\n    \n    # Reshape input\n    n_features = len(w)\n    w = w.reshape((n_features, 1))\n\n    # Compute z-value\n    z = np.matmul(X, w) + b\n\n    return z\n\n\ndef logistic_activation(X, w, b):\n    \"\"\"\n    Runs z-values through a logistic activation function\n\n    Parameters\n    ----------\n    \n    X: ndarray\n        2-D ndarray of datapoints (observations, features)\n    w: ndarray\n        1-D ndarray of feature coefficients\n    b: float\n        Intercept \n\n    Returns\n    -------\n    y_hat: ndarray\n        2-D column vector, prediction of logistic regression (observations, 1)\n    \"\"\"\n    # Calculate z-values\n    z = z_value(X, w, b)\n\n    # Define logistic activation and compute y_hat\n    y_hat = 1/(1 + np.exp(-z))\n\n    # Clip allowable ranges of y_hat, so that floating point rounding to 1 doesn't occur\n    y_hat = np.clip(y_hat, 1e-15, 1 - 1e-15)\n\n    return y_hat\n\n\ndef logistic_cost(X, y, w, b):\n    \"\"\"\n    Calculates cost of logistic regression model\n\n    Parameters\n    ----------\n    X: ndarray\n        2-D ndarray of datapoints (observations, features)\n    y: ndarray\n        2-D column vector, ground truth (observations, 1)\n    w: ndarray\n        1-D ndarray of feature coefficients\n    b: float\n        Intercept \n\n    Returns\n    -------\n    cost: float\n        Logistic cost\n    \"\"\"\n    # Compute predictions\n    y_hat = logistic_activation(X, w, b)\n\n    # Unravel arrays for dot product\n    y = y.flatten()\n    y_hat = y_hat.flatten()\n\n    # Compute cost\n    n_obs = len(y)\n    cost = (-1/n_obs) * (np.dot(y, np.log(y_hat)) + np.dot((1-y), np.log(1-y_hat)))\n\n    return cost\n\ndef gradient_w(X, y, y_hat):\n    \"\"\"\n    Calculate the gradient by logistic loss of feature coefficients\n\n    Parameters\n    ----------\n    X: ndarray\n        2-D ndarray, datapoints (observations, features)\n    y: ndarray\n        2-D column vector, ground truth (observations, 1)\n    y_hat: ndarray\n        2-D column vector, predictions (observations, 1)\n\n    Returns\n    -------\n    w_gradient: ndarray\n        2-D column vector, gradient for coefficients (features, 1)\n    \"\"\"\n    n_obs = y.shape[0]\n    diff = y_hat - y\n    w_gradient = (1/n_obs) * np.matmul(X.T, diff)\n    \n    return w_gradient\n\n    \n   \ndef gradient_b(y, y_hat):\n    \"\"\"\n    Calculate the gradient by logistic loss of feature coefficients\n\n    Parameters\n    ----------\n    y: ndarray\n        2-D column vector, ground truth (observations, 1)\n    y_hat: ndarray\n        2-D column vector, predictions (observations, 1)\n\n    Returns\n    -------\n    b_gradient: float\n        Gradient for intercept\n    \"\"\"\n    n_obs = y.shape[0]\n    diff = y_hat - y\n    b_gradient = (1/n_obs) * np.sum(diff)\n    \n    return b_gradient\n\n\ndef gradient_descent(X, y, w, b, alpha = 10**-2, thres = 10**-5):\n    \"\"\"\n    Runs gradiend descent algorithm\n\n    Parameters\n    ----------\n    X: ndarray\n        2-D ndarray, datapoints (observations, features)\n    y: ndarray\n        2-D column vector, ground truth (observations, 1)\n    w: ndarray\n        1-D vector, initial coefficients\n    b: float\n        Initial intercept\n\n    Returns\n    -------\n    w_hat: ndarray\n        1-D vector, estimated coefficients\n    b_hat: float\n        Estimated intercept\n    \"\"\"\n\n    converged = False\n    cost_history = [logistic_cost(X, y, w, b)]\n    while not converged:\n        # Make predictions with current parameters\n        y_hat = logistic_activation(X, w, b)\n\n        # Calculate gradients and update parameters\n        w = w - (alpha * gradient_w(X, y, y_hat)).flatten()\n        b = b - (alpha * gradient_b(y, y_hat))\n         \n        # Calculate cost \n        cost = logistic_cost(X, y, w, b)\n        cost_history.append(cost)\n        if cost_history[-2] < cost_history[-1]:\n            print(\"Diverged\")\n            return w, b\n        elif cost_history[-2] - cost_history[-1] < thres:\n            converged = True\n    \n    return (w, b)\n\ndef plot_logistic_boundary(X, y, feature_idx, w, b):\n    \"\"\"\n    Creates a plot of two features of a set of data, their labels, and the decision boundary given the features selected and the parameters\n\n    Parameters\n    ----------\n    X: ndarray\n        2-D ndarray, dataset (observations, features)\n    y: ndarray\n        2-D column vector, ground truth (observations, 1)\n    feature_idx: tuple\n        Tuple of integers, column indices to select features from dataset and select feature coefficients\n    w: ndarray\n        1-D vector, feature coefficients\n    b: float\n        Intercept\n    \"\"\"\n    # Plot datapoints\n    feature_x = X[:, feature_idx[0]]\n    feature_y = X[:, feature_idx[1]]\n    plt.scatter(feature_x, feature_y, c = y)\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n\n    # Plot decision boundary\n    x_1 = np.linspace(min(feature_x), max(feature_x), 100)\n    x_2 = (-x_1 * w[feature_idx[0]] - b) / w[feature_idx[1]]\n    plt.plot(x_1, x_2, color = \"blue\")\n    plt.show()\n\n    return None\n\n\ndef z_score_scaling(X):\n    \"\"\"\n    Normalize features of a dataframe with z-score scaling\n    \"\"\"\n    for i in range(X.shape[1]):\n        feature = X[:, i]\n        normalized_feature = (feature - np.mean(feature)) / np.std(feature)\n        X[:, i] = normalized_feature\n\n    return X\n\nif __name__==\"__main__\":\n    \n    ### Testing\n    X_test = np.array([\n        [1, 2],\n        [4, 5],\n        [7, 8],\n        [11, 22],\n        [0.5, 3]])\n    y_test = np.array([\n        [1], \n        [1], \n        [0],\n        [0],\n        [1]])\n\n    # Normalize X\n    X_test = z_score_scaling(X_test)\n    \n    # Compute parameters, and plot decision boundary\n    w = np.array([1, 2])\n    b = 7.65\n    w_hat, b_hat = gradient_descent(X_test, y_test, w, b)\n    preds = logistic_activation(X_test, w_hat, b_hat)\n    plot_logistic_boundary(X_test, y_test, (0, 1), w_hat, b_hat)\n", "type": "text"}, {"name": "shared.py", "content": "import numpy as np\nimport pandas as pd\nfrom gradient_descent import gradient_descent\n\n\n\n####################################\n# Data Structuring Functionalities #\n####################################\ndef dummies(feature):\n    # Assuming that no feature contains multiple datatypes\n    if feature.dtype == \"object\":\n        structured_feature = pd.get_dummies(feature)\n        return structured_feature\n    elif feature.dtype == \"int\" and set(feature.unique()) != {0, 1}:\n        # This assumes that discrete numericals are NOT ordinal!\n        structured_feature = pd.get_dummies(feature)\n        return structured_feature\n    else:\n        return None\n\n\ndef drop_features(df, features):\n    df = df.copy(deep=True)\n    df = df.drop(columns = features)\n    return df\n\n\ndef read_structure_data(df, y_name, columns_to_drop, columns_to_one_hot):\n\n    # Drop unwanted columns\n    df = df.drop(columns = columns_to_drop)\n\n    # Drop Na rows (must be completed before feature selection, on the complete dataframe)\n    df = df.dropna(how=\"any\")\n\n    # Transform categorical features into one-hot\n    col_names = df.columns\n    for col in columns_to_one_hot:\n        dummies = pd.get_dummies(df[col], prefix = col + \"_\")    \n        df = df.drop(columns = col)\n        df = pd.concat((df, dummies), axis = 1)\n\n    # z-score normalize all columns except y\n    y = df.pop(y_name)\n    df = df.apply(lambda x: (x - np.mean(x))/np.std(x), axis = 0)\n\n    # Transform into numpy arrays\n    n_obs = len(y)\n    y = np.array(y).reshape((n_obs, 1))\n    X = np.array(df)\n\n    # Reassemble Dataframe\n    y_series = pd.Series(y.flatten())\n    y_series.index = df.index\n    df[y_name] = y_series\n\n    return df\n\n\n# Extract available features into a dictionary\ndef extract_feature_names(df):\n    feature_names = df.columns\n    features_dict = {}\n    features_type = []\n    for name in feature_names:\n        features_dict[name] = name\n    return features_dict\n\ndef transform_array(df, y_idx):\n    \"\"\"\n    Transforms a dataframe into numpy arrays X and y\n    \"\"\"\n    X_array = np.array(df)\n    y_array = X_array[:, y_idx].reshape((X_array.shape[0], 1))\n    X_array = np.delete(X_array, y_idx, axis = 1)\n    \n    return X_array, y_array\n\ndef product_remainder(X, w, feature_idx):\n    \"\"\"\n    Compute dot product of remaining features and coefficients, that are not to be plotted\n    \"\"\"\n    features = np.delete(X, feature_idx, axis = 1)\n    weights = np.delete(w, feature_idx)\n    av = np.mean(features, axis = 0).flatten()\n    product = np.dot(av, weights)\n\n    return product\n\n\n\n", "type": "text"}]